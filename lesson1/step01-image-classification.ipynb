{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle大赛第一步\n",
    "前段时间学完了Udacity的深度学习基石课程，内容挺好，但是就是缺乏实战，于是找到了fast.ai的课程，准备进入kaggle大赛来检验下自己的学习成果，这是kaggle大赛第一步：入门。\n",
    "\n",
    "下面是一些TODO LIST\n",
    "\n",
    "1. 注册Kaggle的账号\n",
    "2. 选择[dogs-vs-cats-redux-kernels-edition](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition)项目，选择同意数据协议（为了后面通过命令行下载数据用）\n",
    "3. [aws环境搭建](http://wiki.fast.ai/index.php/AWS_install)\n",
    "4. 数据目录准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download data\n",
    "第一步，我们要去下载数据，地址：链接: https://pan.baidu.com/s/1kUAnr3T 密码: pdxw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhuanxu/PycharmProjects/udacity/fast-ai/lesson1/../data/redux\n",
      "/Users/zhuanxu/PycharmProjects/udacity/fast-ai/lesson1\n"
     ]
    }
   ],
   "source": [
    "# frequent file shortcuts\n",
    "import os, sys\n",
    "current_directory = os.getcwd()\n",
    "LESSON_HOME_DIR = current_directory\n",
    "DATA_HOME_DIR = os.path.join(current_directory,\"../data/redux\")\n",
    "print(DATA_HOME_DIR)\n",
    "print(LESSON_HOME_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 创建目录\n",
    "![ ! -d $DATA_HOME_DIR ] && mkdir -pv $DATA_HOME_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhuanxu/PycharmProjects/udacity/fast-ai/data/redux\n"
     ]
    }
   ],
   "source": [
    "#Create directories\n",
    "%cd $DATA_HOME_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 命令安装\n",
    "# 参考文档：https://github.com/floydwch/kaggle-cli\n",
    "# !pip install kaggle-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 配置kg\n",
    "# 参考：http://wiki.fast.ai/index.php/Kaggle_CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize data into folders\n",
    "这一步是为了更好的组织我们的数据，结构如下：\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhuanxu/PycharmProjects/udacity/fast-ai/data/redux\n"
     ]
    }
   ],
   "source": [
    "#Create directories\n",
    "%cd $DATA_HOME_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir -pv valid # 验证\n",
    "%mkdir results # 结果\n",
    "%mkdir -pv sample/train # 测试用目录\n",
    "%mkdir -pv sample/test\n",
    "%mkdir -pv sample/valid\n",
    "%mkdir -pv sample/results\n",
    "%mkdir -pv test/unknown # 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir results\n",
    "# !mkdir -pv valid\n",
    "# !mkdir -pv sample/train\n",
    "# !mkdir -pv sample/test\n",
    "# !mkdir -pv sample/valid\n",
    "# !mkdir -pv sample/results\n",
    "# !mkdir -pv test/unknown # 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhuanxu/PycharmProjects/udacity/fast-ai/data/redux/train\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机移动2k张照片到验证目录\n",
    "import glob\n",
    "import numpy as np\n",
    "g = glob.glob('*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(100): os.rename(shuf[i], DATA_HOME_DIR+'/valid/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     100\r\n"
     ]
    }
   ],
   "source": [
    "!ls $DATA_HOME_DIR/valid | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "# copy 10 张到 sample的train目录下\n",
    "from shutil import copyfile\n",
    "g = glob.glob('*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(100): copyfile(shuf[i], DATA_HOME_DIR+'/sample/train/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $DATA_HOME_DIR/sample/train/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = glob.glob('*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(200): copyfile(shuf[i], DATA_HOME_DIR+'/sample/valid/' + shuf[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rearrange image files into their respective directories\n",
    "重新安排目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhuanxu/PycharmProjects/udacity/fast-ai/data/redux/sample/train\n",
      "mv: rename cat.*.jpg to cats/cat.*.jpg: No such file or directory\n",
      "mv: rename dog.*.jpg to dogs/dog.*.jpg: No such file or directory\n",
      "/Users/zhuanxu/PycharmProjects/udacity/fast-ai/data/redux/sample/valid\n",
      "mv: rename cat.*.jpg to cats/cat.*.jpg: No such file or directory\n",
      "mv: rename dog.*.jpg to dogs/dog.*.jpg: No such file or directory\n",
      "/Users/zhuanxu/PycharmProjects/udacity/fast-ai/data/redux/valid\n",
      "/Users/zhuanxu/PycharmProjects/udacity/fast-ai/data/redux/train\n"
     ]
    }
   ],
   "source": [
    "#Divide cat/dog images into separate directories\n",
    "\n",
    "%cd $DATA_HOME_DIR/sample/train\n",
    "%mkdir cats\n",
    "%mkdir dogs\n",
    "%mv cat.*.jpg cats/\n",
    "%mv dog.*.jpg dogs/\n",
    "\n",
    "%cd $DATA_HOME_DIR/sample/valid\n",
    "%mkdir cats\n",
    "%mkdir dogs\n",
    "%mv cat.*.jpg cats/\n",
    "%mv dog.*.jpg dogs/\n",
    "\n",
    "%cd $DATA_HOME_DIR/valid\n",
    "%mkdir cats\n",
    "%mkdir dogs\n",
    "%mv cat.*.jpg cats/\n",
    "%mv dog.*.jpg dogs/\n",
    "\n",
    "%cd $DATA_HOME_DIR/train\n",
    "%mkdir cats\n",
    "%mkdir dogs\n",
    "%mv cat.*.jpg cats/\n",
    "%mv dog.*.jpg dogs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhuanxu/PycharmProjects/udacity/fast-ai/data/redux/test\n"
     ]
    }
   ],
   "source": [
    "# Create single 'unknown' class for test set\n",
    "%cd $DATA_HOME_DIR/test\n",
    "%mv *.jpg unknown/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhuanxu/PycharmProjects/udacity/fast-ai/data/redux\n",
      ".\r\n",
      "├── results\r\n",
      "├── sample\r\n",
      "│   ├── results\r\n",
      "│   ├── test\r\n",
      "│   ├── train\r\n",
      "│   │   ├── cats\r\n",
      "│   │   └── dogs\r\n",
      "│   └── valid\r\n",
      "│       ├── cats\r\n",
      "│       └── dogs\r\n",
      "├── test\r\n",
      "│   └── unknown\r\n",
      "├── train\r\n",
      "│   ├── cats\r\n",
      "│   └── dogs\r\n",
      "└── valid\r\n",
      "    ├── cats\r\n",
      "    └── dogs\r\n",
      "\r\n",
      "18 directories\r\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "!tree -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型\n",
    "此处我们没有直接采用fast.ai中的文件，而是决定自己来重新写下，为什么呢？一是为了让自己更好的理解，另一个则是使用Python版本的原因，此处我们使用3.5版本\n",
    "\n",
    "第一步还是先直接饮用课程中写好的文件，直接运行，看有什么问题的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhuanxu/PycharmProjects/udacity/fast-ai/data/redux\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "\n",
    "#Set path to sample/ path if desired\n",
    "path = DATA_HOME_DIR + '/' #'/sample/'\n",
    "test_path = DATA_HOME_DIR + '/test/' #We use all the test data\n",
    "results_path=DATA_HOME_DIR + '/results/'\n",
    "train_path=path + '/train/'\n",
    "valid_path=path + '/valid/'\n",
    "utils_path = current_directory + \"/../utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# !KERAS_BACKEND=theano\n",
    "from keras import backend\n",
    "backend.set_image_dim_ordering(\"th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backend.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuanxu/anaconda/envs/linear_regression_demo/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: No GPU found. Please use a GPU to train your neural network.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allow relative imports to directories above lesson1/\n",
    "sys.path.insert(1, os.path.join(utils_path))\n",
    "#import modules\n",
    "from utils import *\n",
    "from vgg16 import Vgg16\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面遇到的问题，由于是3.5版本，需要修改\n",
    "```\n",
    "#import cPickle as pickle\n",
    "import _pickle as pickle\n",
    "```\n",
    "conda install bcolz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import vgg16\n",
    "importlib.reload(vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处有问题，去看下get_batches里面到底做了什么呢？里面有个重要的函数`ImageDataGenerator`，让我们来看下到底是干什么了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "gen = ImageDataGenerator( rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面其实是通过 ImageDataGenerator 来做了一个数据的增强Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "![ ! -d $path/preview ] && mkdir $path/preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = gen.flow_from_directory(train_path,target_size=(224,224),class_mode='categorical',shuffle=False,batch_size=1,\n",
    "                                  save_to_dir=path+'/preview', save_prefix='dog-cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flow.class_indices) # 类别字典\n",
    "print(len(flow.classes),flow.classes) # 加载进来的标签\n",
    "print(flow.image_shape) # 图片大小\n",
    "print(flow.nb_class) # 类别个数\n",
    "print(flow.nb_sample) # 总个数\n",
    "print(flow.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "flow.reset()\n",
    "for batch in flow:\n",
    "    i+=1\n",
    "    # batch is  a tuple (inputs, targets)\n",
    "    print(type(batch),len(batch),type(batch[0]),batch[0].shape,batch[1].shape,batch[1]) # 具体的图像数据\n",
    "    if i>8:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "prview_path = path+'/preview/'\n",
    "imagefiles = [ f for f in listdir(prview_path) if isfile(join(prview_path,f)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "#Helper function to plot images by index in the validation set \n",
    "#Plots is a helper function in utils.py\n",
    "def plots_idx(f, titles=None):\n",
    "    plots([image.load_img(join(prview_path,i)) for i in f], titles=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_idx(imagefiles[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步，我们需要看的函数是 finetune 函数，看看里面到底做了什么\n",
    "\n",
    "原vgg模型最后一层是一个输出为1000的FC层，此处我们需要将其去掉，然后改为我们需要的2分类器，代码如下：\n",
    "```\n",
    "def ft(self, num):\n",
    "        \"\"\"\n",
    "            Replace the last layer of the model with a Dense (fully connected) layer of num neurons.\n",
    "            Will also lock the weights of all layers except the new layer so that we only learn\n",
    "            weights for the last layer in subsequent training.\n",
    "\n",
    "            Args:\n",
    "                num (int) : Number of neurons in the Dense layer\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "        model = self.model\n",
    "        model.pop()\n",
    "        for layer in model.layers: layer.trainable=False\n",
    "        model.add(Dense(num, activation='softmax'))\n",
    "        self.compile()\n",
    "```\n",
    "\n",
    "因此 finetune 中做的事情就是重新构建了下模型\n",
    "\n",
    "接着到了最关键的 fit 方法了，具体训练模型了，方法可以参看[model](https://keras.io/models/model/)，fit 和 fit_generator 的区别是输入是否是 generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Signature: model.fit_generator(generator, samples_per_epoch, nb_epoch, \n",
    "#   verbose=1, callbacks=None, validation_data=None, nb_val_samples=None, class_weight=None,\n",
    "#   max_q_size=10, nb_worker=1, pickle_safe=False, initial_epoch=0, **kwargs)\n",
    "# model.fit_generator()\n",
    "# 此处使用的 keras 的版本比较老，所有用的是老版本的接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Vgg16 helper class\n",
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set constants. You can experiment with no_of_epochs to improve the model\n",
    "batch_size=8\n",
    "no_of_epochs=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finetune the model\n",
    "batches = vgg.get_batches(train_path, batch_size=batch_size)\n",
    "val_batches = vgg.get_batches(valid_path, batch_size=batch_size*2)\n",
    "vgg.finetune(batches)\n",
    "\n",
    "#Not sure if we set this for all fits\n",
    "vgg.model.optimizer.lr = 0.1\n",
    "# vgg.model.compile(optimizer=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 换一个优化方法\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "opt = RMSprop(lr=0.1)\n",
    "vgg.model.compile(optimizer=opt,\n",
    "                loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# 此处优化算法换后，提升效果特别明显"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice we are passing in the validation dataset to the fit() method\n",
    "#For each epoch we test our model against the validation set\n",
    "latest_weights_filename = None\n",
    "for epoch in range(no_of_epochs):\n",
    "    print(\"Running epoch: %d\" %(epoch))\n",
    "    vgg.fit(batches, val_batches, nb_epoch=1)\n",
    "    latest_weights_filename = 'ft%d.h5' % epoch\n",
    "    vgg.model.save_weights(results_path+latest_weights_filename)\n",
    "print(\"Completed %s fit operations\"%(no_of_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处有个大坑，我一上来就用了这么大的照片来训练，耗时严重，还不好调节参数，我们应该现在sample上验证完后，我们再放到真实数据上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches, preds = vgg.test(test_path, batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For every image, vgg.test() generates two probabilities \n",
    "#based on how we've ordered the cats/dogs directories.\n",
    "#It looks like column one is cats and column two is dogs\n",
    "print(preds[:5])\n",
    "\n",
    "filenames = batches.filenames\n",
    "print(filenames[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can verify the column ordering by viewing some images\n",
    "from PIL import Image\n",
    "Image.open(test_path + filenames[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save our test results arrays so we can use them again later\n",
    "save_array(results_path + 'test_preds.dat', preds)\n",
    "save_array(results_path + 'filenames.dat', filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Predictions\n",
    "模型验证，我们主要看下面5类数据：\n",
    "\n",
    "1. A few correct labels at random\n",
    "2. A few incorrect labels at random\n",
    "3. The most correct labels of each class (ie those with highest probability that are correct)\n",
    "4. The most incorrect labels of each class (ie those with highest probability that are incorrect)\n",
    "5. The most uncertain labels (ie those with probability closest to 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.load_weights(results_path+latest_weights_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches, probs = vgg.test(valid_path, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = val_batches.filenames\n",
    "expected_labels = val_batches.classes #0 or 1\n",
    "\n",
    "#Round our predictions to 0/1 to generate labels\n",
    "our_predictions = probs[:,0] # 第一列都是表示猫的\n",
    "our_labels = np.round(1-our_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "#Helper function to plot images by index in the validation set \n",
    "#Plots is a helper function in utils.py\n",
    "def plots_idx(idx, titles=None):\n",
    "    plots([image.load_img(valid_path + filenames[i]) for i in idx], titles=titles)\n",
    "    \n",
    "#Number of images to view for each visualization task\n",
    "n_view = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(expected_labels, our_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, val_batches.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Predictions to Kaggle!\n",
    "提交的形式是：\n",
    "```\n",
    "imageId,isDog\n",
    "1242, .3984\n",
    "3947, .1000\n",
    "4539, .9082\n",
    "2345, .0000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load our test predictions from file\n",
    "preds = load_array(results_path + 'test_preds.dat')\n",
    "filenames = load_array(results_path + 'filenames.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract imageIds from the filenames in our test/unknown directory \n",
    "filenames = batches.filenames\n",
    "ids = np.array([int(f[8:f.find('.')]) for f in filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(ids) == len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = np.stack([ids,isdog], axis=1)\n",
    "subm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "submission_file_name = 'submission1.csv'\n",
    "np.savetxt(submission_file_name, subm, fmt='%d,%.5f', header='id,label', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "%cd $LESSON_HOME_DIR\n",
    "FileLink('../data/redux/'+submission_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题记录\n",
    "\n",
    "1. 在aws环境搭建中，第一个问题是只支持3个地域，离中国太远了。慢，于是想个办法能不能用国内的？所以我换了Udacity-dl的ami，目前只能手动建立，做不到fast.ai中的命令行，暂时没有时间去研究那个脚本\n",
    "\n",
    "2. 在mac上显示清晰问题\n",
    "```\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
